{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check submission pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# 如果不添加该行，则每次显示图片都需要加上plt.show\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images number: 33536\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = 'test_mtcnn_data/'                      # testset dir\n",
    "name_list = [name for name in os.listdir(test_data_dir)]\n",
    "img_paths = [test_data_dir + name for name in os.listdir(test_data_dir)]\n",
    "print('Images number:', len(img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read test dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_sub = open('raw_data/submission_template.csv', 'r')  # sample submission file dir\n",
    "\n",
    "print('Loaded CSV')\n",
    "\n",
    "lines = sample_sub.readlines()\n",
    "random.shuffle(lines)\n",
    "i = 0\n",
    "for line in lines:\n",
    "    pair = line.split(',')[0]\n",
    "    a, b = pair.split(':')\n",
    "    a_pic = plt.imread(test_data_dir + a[:-4]+'_mtcnn.jpg')\n",
    "    print(a , b)\n",
    "#     print(f'第一张图的shape:{a_pic.shape}')\n",
    "    b_pic = plt.imread(test_data_dir + b[:-4]+'_mtcnn.jpg')\n",
    "#     print(f'第二张图的shape:{b_pic.shape}')\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(a_pic)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(b_pic)\n",
    "    plt.show()\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read train data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data_dir = 'raw_data/train_data/'                  # train dir\n",
    "train_csv = open('raw_data/training.csv', 'r')  # sample submission file dir\n",
    "lines = train_csv.readlines()\n",
    "print('Train Images number:', len(lines))\n",
    "random.shuffle(lines)\n",
    "i = 0\n",
    "for line in lines:\n",
    "    jpg_path = line.split(',')[0]\n",
    "    a_pic = plt.imread(train_data_dir + jpg_path)\n",
    "#     print(jpg_path)\n",
    "    if a_pic.shape == (130, 112, 3):\n",
    "        print('find one')\n",
    "        i += 1\n",
    "print(i)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(a_pic)\n",
    "#     plt.show()\n",
    "#     i += 1\n",
    "#     if i == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_a = plt.imread('raw_data/train_data/mtcnn_training/Indian/01459/00003_mtcnn.jpg')\n",
    "img_b = plt.imread('raw_data/train_data/training/Indian/01459/00003.jpg')\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_a)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'raw_data/train_data/'                  # train dir\n",
    "train_csv = open('raw_data/training.csv', 'r')  # sample submission file dir\n",
    "lines = train_csv.readlines()\n",
    "print('Train Images number:', len(lines))\n",
    "random.shuffle(lines)\n",
    "i = 0\n",
    "for line in lines:\n",
    "    \n",
    "    jpg_path = line.split(',')[0]\n",
    "    path_terms = jpg_path.split('/')\n",
    "    \n",
    "    if path_terms[1] != 'Indian':\n",
    "        \n",
    "        continue\n",
    "    print(path_terms[1])\n",
    "    a_pic = plt.imread(train_data_dir + jpg_path)\n",
    "    \n",
    "    mtcnn_path = os.path.join('mtcnn_' + path_terms[0] , path_terms[1] , path_terms[2] , path_terms[3][:-4] +'_mtcnn.jpg')\n",
    "    b_pic = plt.imread(train_data_dir + mtcnn_path)\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(a_pic)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(b_pic)\n",
    "    plt.show()\n",
    "    i += 1\n",
    "#     if i == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b_pic = plt.imread('test_mtcnn_data/Asian_2792_mtcnn.jpg')\n",
    "print(b_pic.shape)\n",
    "plt.figure()\n",
    "# plt.subplot(1,2,1)\n",
    "plt.imshow(b_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV\n",
      "[('Asian_7404.jpg', 1339), ('Asian_9349.jpg', 1339), ('Asian_6108.jpg', 1339), ('Asian_6919.jpg', 1339), ('Asian_9363.jpg', 1339), ('Asian_3414.jpg', 1339), ('Asian_4094.jpg', 1339), ('Asian_1847.jpg', 1339), ('Asian_6371.jpg', 1339), ('Asian_5697.jpg', 1339)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "sample_sub = open('raw_data/submission_template.csv', 'r')  # sample submission file dir\n",
    "\n",
    "print('Loaded CSV')\n",
    "\n",
    "pic_count = Counter()\n",
    "lines = sample_sub.readlines()\n",
    "random.shuffle(lines)\n",
    "i = 0\n",
    "all_pic_list = []\n",
    "for line in lines:\n",
    "    pair = line.split(',')[0]\n",
    "    a, b = pair.split(':')\n",
    "    all_pic_list.append(a)\n",
    "    all_pic_list.append(b)\n",
    "for pic in all_pic_list:\n",
    "    pic_count[pic] += 1\n",
    "print(pic_count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Indian': 8048, 'Caucasian': 8255, 'Asian': 9419, 'African': 7814}\n"
     ]
    }
   ],
   "source": [
    "all_pic_set = set(all_pic_list)\n",
    "\n",
    "count_class = {'Indian':0,\n",
    "                'Caucasian':0,\n",
    "                'Asian':0,\n",
    "                'African':0,\n",
    "              }\n",
    "for pic in all_pic_set:\n",
    "    if pic.split('_')[0] == 'Indian':\n",
    "        count_class['Indian'] += 1\n",
    "    elif pic.split('_')[0] == 'Caucasian':\n",
    "        count_class['Caucasian'] += 1\n",
    "    elif pic.split('_')[0] == 'Asian':\n",
    "        count_class['Asian'] += 1\n",
    "    elif pic.split('_')[0] == 'African':\n",
    "        count_class['African'] += 1\n",
    "print(count_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Indian': 3545022, 'Caucasian': 3423016, 'Asian': 3572346, 'African': 3555664}\n"
     ]
    }
   ],
   "source": [
    "count_class = {'Indian':0,\n",
    "                'Caucasian':0,\n",
    "                'Asian':0,\n",
    "                'African':0,\n",
    "              }\n",
    "for pic in all_pic_list:\n",
    "    if pic.split('_')[0] == 'Indian':\n",
    "        count_class['Indian'] += 1\n",
    "    elif pic.split('_')[0] == 'Caucasian':\n",
    "        count_class['Caucasian'] += 1\n",
    "    elif pic.split('_')[0] == 'Asian':\n",
    "        count_class['Asian'] += 1\n",
    "    elif pic.split('_')[0] == 'African':\n",
    "        count_class['African'] += 1\n",
    "print(count_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "same_line_num = 0\n",
    "for line in lines:\n",
    "    pair = line.split(',')[0]\n",
    "    a, b = pair.split(':')\n",
    "    if a == b:\n",
    "        same_line_num += 1\n",
    "print(same_line_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33536\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'test_data'\n",
    "pic_list = os.listdir(path)\n",
    "print(len(pic_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33536\n"
     ]
    }
   ],
   "source": [
    "dst_path = 'test_mask_datapics/test_data/'\n",
    "pic_list_mask = os.listdir(dst_path)\n",
    "print(len(pic_list_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = list(set(pic_list) ^ set(pic_list_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['African_2380.jpg',\n",
       " 'Asian_7331.jpg',\n",
       " 'African_2549.jpg',\n",
       " 'African_4482.jpg',\n",
       " 'Asian_8660.jpg',\n",
       " 'African_1878.jpg',\n",
       " 'African_4754.jpg',\n",
       " 'Asian_8349.jpg',\n",
       " 'Indian_0536.jpg',\n",
       " 'African_2397.jpg',\n",
       " 'Asian_1414.jpg',\n",
       " 'African_3211.jpg',\n",
       " 'Asian_6535.jpg',\n",
       " 'African_1400.jpg',\n",
       " 'Indian_1857.jpg',\n",
       " 'African_5677.jpg',\n",
       " 'African_1437.jpg',\n",
       " 'Indian_1588.jpg',\n",
       " 'Indian_4498.jpg',\n",
       " 'African_2679.jpg',\n",
       " 'Asian_2809.jpg',\n",
       " 'Indian_7462.jpg',\n",
       " 'Asian_3959.jpg',\n",
       " 'Asian_1989.jpg',\n",
       " 'African_4350.jpg',\n",
       " 'Asian_5325.jpg',\n",
       " 'Asian_5860.jpg',\n",
       " 'African_1651.jpg',\n",
       " 'Asian_3156.jpg',\n",
       " 'African_5490.jpg',\n",
       " 'Indian_5956.jpg',\n",
       " 'African_4567.jpg',\n",
       " 'African_6494.jpg',\n",
       " 'Indian_6003.jpg',\n",
       " 'Asian_4304.jpg',\n",
       " 'Asian_4117.jpg',\n",
       " 'Caucasian_0524.jpg',\n",
       " 'African_2538.jpg',\n",
       " 'Asian_4269.jpg',\n",
       " 'African_5148.jpg',\n",
       " 'African_2994.jpg',\n",
       " 'African_1655.jpg',\n",
       " 'African_2232.jpg',\n",
       " 'Asian_2865.jpg',\n",
       " 'African_1060.jpg',\n",
       " 'African_2595.jpg',\n",
       " 'Indian_5520.jpg',\n",
       " 'Indian_1338.jpg',\n",
       " 'African_4045.jpg',\n",
       " 'African_4850.jpg',\n",
       " 'African_7170.jpg',\n",
       " 'Indian_7818.jpg',\n",
       " 'Asian_3099.jpg',\n",
       " 'African_6655.jpg',\n",
       " 'Asian_8692.jpg',\n",
       " 'Asian_5284.jpg',\n",
       " 'African_1246.jpg',\n",
       " 'Caucasian_5508.jpg',\n",
       " 'Asian_3246.jpg',\n",
       " 'African_2460.jpg',\n",
       " 'Asian_2408.jpg',\n",
       " 'Indian_1242.jpg',\n",
       " 'African_6897.jpg',\n",
       " 'Asian_2409.jpg',\n",
       " 'Indian_0022.jpg',\n",
       " 'Asian_0531.jpg',\n",
       " 'Caucasian_1583.jpg',\n",
       " 'African_2805.jpg',\n",
       " 'Caucasian_6279.jpg',\n",
       " 'African_5246.jpg',\n",
       " 'African_3310.jpg',\n",
       " 'Asian_5957.jpg',\n",
       " 'Caucasian_6789.jpg',\n",
       " 'African_7320.jpg',\n",
       " 'African_2638.jpg',\n",
       " 'Asian_3841.jpg',\n",
       " 'Indian_5269.jpg',\n",
       " 'African_6542.jpg',\n",
       " 'Asian_6717.jpg',\n",
       " 'African_1957.jpg',\n",
       " 'Asian_2343.jpg',\n",
       " 'Asian_5443.jpg',\n",
       " 'African_7701.jpg',\n",
       " 'African_5198.jpg',\n",
       " 'Asian_4933.jpg',\n",
       " 'African_2459.jpg',\n",
       " 'African_4345.jpg',\n",
       " 'Asian_2025.jpg',\n",
       " 'Asian_0567.jpg',\n",
       " 'African_3773.jpg',\n",
       " 'Asian_6031.jpg',\n",
       " 'Asian_6644.jpg',\n",
       " 'Indian_1605.jpg',\n",
       " 'Indian_3241.jpg',\n",
       " 'African_0242.jpg',\n",
       " 'African_1485.jpg',\n",
       " 'African_2551.jpg',\n",
       " 'Asian_6441.jpg',\n",
       " 'African_2718.jpg',\n",
       " 'Asian_7963.jpg',\n",
       " 'Indian_1757.jpg',\n",
       " 'African_3410.jpg',\n",
       " 'Asian_7750.jpg',\n",
       " 'Asian_0124.jpg',\n",
       " 'African_0345.jpg',\n",
       " 'Asian_9267.jpg',\n",
       " 'Asian_3974.jpg',\n",
       " 'African_6410.jpg',\n",
       " 'Caucasian_7989.jpg',\n",
       " 'Caucasian_2635.jpg',\n",
       " 'Asian_5866.jpg',\n",
       " 'Asian_1393.jpg',\n",
       " 'Indian_5142.jpg',\n",
       " 'Caucasian_6529.jpg',\n",
       " 'Asian_8882.jpg',\n",
       " 'Asian_2656.jpg',\n",
       " 'African_0929.jpg',\n",
       " 'African_0160.jpg',\n",
       " 'Indian_1500.jpg',\n",
       " 'African_0934.jpg',\n",
       " 'Asian_1560.jpg',\n",
       " 'Indian_0997.jpg',\n",
       " 'Asian_7352.jpg',\n",
       " 'Caucasian_2731.jpg',\n",
       " 'Indian_4462.jpg',\n",
       " 'Asian_0254.jpg',\n",
       " 'Indian_3033.jpg',\n",
       " 'Asian_9155.jpg',\n",
       " 'Indian_5890.jpg',\n",
       " 'Asian_6873.jpg',\n",
       " 'Caucasian_5722.jpg',\n",
       " 'Asian_8428.jpg',\n",
       " 'Asian_7514.jpg',\n",
       " 'African_3764.jpg',\n",
       " 'African_4674.jpg',\n",
       " 'Asian_1261.jpg',\n",
       " 'African_1811.jpg',\n",
       " 'African_2100.jpg',\n",
       " 'African_5365.jpg',\n",
       " 'Caucasian_0251.jpg',\n",
       " 'African_4062.jpg',\n",
       " 'Asian_0637.jpg',\n",
       " 'Asian_2743.jpg',\n",
       " 'African_6821.jpg',\n",
       " 'Indian_1728.jpg',\n",
       " 'Asian_2833.jpg',\n",
       " 'African_1594.jpg',\n",
       " 'African_7161.jpg',\n",
       " 'African_0775.jpg',\n",
       " 'Asian_4690.jpg',\n",
       " 'Asian_9208.jpg',\n",
       " 'Asian_5314.jpg',\n",
       " 'African_3683.jpg',\n",
       " 'African_6184.jpg',\n",
       " 'Caucasian_7258.jpg',\n",
       " 'Asian_3623.jpg',\n",
       " 'African_6312.jpg',\n",
       " 'Asian_0718.jpg',\n",
       " 'Indian_3311.jpg',\n",
       " 'Caucasian_7680.jpg',\n",
       " 'African_0770.jpg',\n",
       " 'African_7150.jpg',\n",
       " 'Asian_7053.jpg',\n",
       " 'Indian_3965.jpg',\n",
       " 'Indian_3025.jpg',\n",
       " 'African_0219.jpg',\n",
       " 'Caucasian_6636.jpg',\n",
       " 'African_4022.jpg',\n",
       " 'Caucasian_0498.jpg',\n",
       " 'African_7335.jpg',\n",
       " 'African_2162.jpg',\n",
       " 'African_4050.jpg',\n",
       " 'Asian_0081.jpg',\n",
       " 'Asian_7035.jpg',\n",
       " 'African_1508.jpg',\n",
       " 'African_6716.jpg',\n",
       " 'African_1844.jpg',\n",
       " 'Caucasian_3921.jpg',\n",
       " 'African_4483.jpg',\n",
       " 'Asian_1346.jpg',\n",
       " 'Asian_2148.jpg',\n",
       " 'Indian_2487.jpg',\n",
       " 'Indian_3972.jpg',\n",
       " 'African_0508.jpg',\n",
       " 'Caucasian_5944.jpg',\n",
       " 'Asian_1696.jpg',\n",
       " 'Indian_1932.jpg',\n",
       " 'Indian_5094.jpg',\n",
       " 'Asian_8672.jpg',\n",
       " 'Caucasian_2941.jpg',\n",
       " 'Asian_9061.jpg',\n",
       " 'Asian_1932.jpg',\n",
       " 'Caucasian_4649.jpg',\n",
       " 'African_2088.jpg',\n",
       " 'African_1641.jpg',\n",
       " 'Asian_6685.jpg',\n",
       " 'Indian_0763.jpg',\n",
       " 'Asian_7267.jpg',\n",
       " 'Caucasian_1792.jpg',\n",
       " 'African_4984.jpg',\n",
       " 'African_4452.jpg',\n",
       " 'African_4763.jpg',\n",
       " 'Indian_5826.jpg',\n",
       " 'Asian_3447.jpg',\n",
       " 'African_4073.jpg',\n",
       " 'Indian_7289.jpg',\n",
       " 'African_1768.jpg',\n",
       " 'Indian_0666.jpg',\n",
       " 'Asian_3504.jpg',\n",
       " 'African_7249.jpg',\n",
       " 'Asian_1921.jpg',\n",
       " 'African_3973.jpg',\n",
       " 'African_4886.jpg',\n",
       " 'Caucasian_0531.jpg',\n",
       " 'Indian_7031.jpg',\n",
       " 'Asian_2697.jpg',\n",
       " 'African_7196.jpg',\n",
       " 'Asian_8020.jpg',\n",
       " 'Asian_6117.jpg',\n",
       " 'African_2205.jpg',\n",
       " 'African_7284.jpg',\n",
       " 'Asian_7042.jpg',\n",
       " 'African_3247.jpg',\n",
       " 'Asian_7887.jpg',\n",
       " 'African_2778.jpg',\n",
       " 'Asian_4946.jpg',\n",
       " 'African_5753.jpg',\n",
       " 'African_1114.jpg',\n",
       " 'Indian_0200.jpg',\n",
       " 'Asian_8195.jpg',\n",
       " 'Asian_4973.jpg',\n",
       " 'African_5262.jpg',\n",
       " 'African_6413.jpg',\n",
       " 'African_3222.jpg',\n",
       " 'African_1818.jpg',\n",
       " 'Asian_7676.jpg',\n",
       " 'African_7484.jpg',\n",
       " 'African_7702.jpg',\n",
       " 'Indian_5538.jpg',\n",
       " 'African_2124.jpg',\n",
       " 'African_0209.jpg',\n",
       " 'Asian_5403.jpg',\n",
       " 'Asian_1811.jpg',\n",
       " 'African_4250.jpg',\n",
       " 'Indian_3883.jpg',\n",
       " 'African_0327.jpg',\n",
       " 'African_4002.jpg',\n",
       " 'African_6000.jpg',\n",
       " 'Asian_4816.jpg',\n",
       " 'African_1568.jpg',\n",
       " 'Asian_8783.jpg',\n",
       " 'Indian_0515.jpg',\n",
       " 'Asian_2002.jpg',\n",
       " 'Asian_6975.jpg',\n",
       " 'African_5972.jpg',\n",
       " 'African_7547.jpg',\n",
       " 'African_5597.jpg',\n",
       " 'Asian_7850.jpg',\n",
       " 'Indian_0299.jpg',\n",
       " 'Asian_7349.jpg',\n",
       " 'Indian_5128.jpg',\n",
       " 'Asian_8861.jpg',\n",
       " 'African_6553.jpg',\n",
       " 'Caucasian_2122.jpg',\n",
       " 'African_0604.jpg',\n",
       " 'Asian_2050.jpg',\n",
       " 'African_5835.jpg',\n",
       " 'Indian_0891.jpg',\n",
       " 'African_6581.jpg',\n",
       " 'Asian_0606.jpg',\n",
       " 'Indian_3349.jpg',\n",
       " 'Caucasian_1577.jpg',\n",
       " 'African_6055.jpg',\n",
       " 'Asian_3926.jpg',\n",
       " 'Asian_1345.jpg',\n",
       " 'African_5225.jpg',\n",
       " 'African_5562.jpg',\n",
       " 'Asian_3742.jpg',\n",
       " 'Asian_1497.jpg',\n",
       " 'Asian_9286.jpg',\n",
       " 'African_0970.jpg',\n",
       " 'African_4443.jpg',\n",
       " 'Indian_6427.jpg',\n",
       " 'Asian_8026.jpg',\n",
       " 'Asian_7404.jpg',\n",
       " 'Asian_3252.jpg',\n",
       " 'Asian_5405.jpg',\n",
       " 'Indian_2665.jpg',\n",
       " 'Indian_2096.jpg',\n",
       " 'Asian_5940.jpg',\n",
       " 'African_0578.jpg',\n",
       " 'Caucasian_6823.jpg',\n",
       " 'African_6820.jpg',\n",
       " 'Asian_4913.jpg',\n",
       " 'African_0999.jpg',\n",
       " 'Asian_1350.jpg',\n",
       " 'Indian_2033.jpg',\n",
       " 'Asian_6129.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for item in ret:\n",
    "    shutil.copy(path +'/'+ item , dst_path + item)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='face model test')\n",
    "# general\n",
    "parser.add_argument('--image-size', default='112,112', help='')\n",
    "parser.add_argument('--model', default='../../pretrained_model/model-r100-ii/model,0', help='path to load model.')\n",
    "parser.add_argument('--ga-model', default='', help='path to load model.')\n",
    "parser.add_argument('--gpu', default=0, type=int, help='gpu id')\n",
    "parser.add_argument('--det', default=0, type=int, help='mtcnn option, 1 means using R+O, 0 means detect from begining')\n",
    "parser.add_argument('--flip', default=0, type=int, help='whether do lr flip aug')\n",
    "parser.add_argument('--threshold', default=1.24, type=float, help='ver dist threshold')\n",
    "parser.add_argument('--featured_mat', default='', type=str, help='did calculate features')\n",
    "parser.add_argument('--test_data_mtcnn', default=0, type=int, help='do test data needs transfrom')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_class_feature_dict = {'Indian':{},\n",
    "                'Caucasian':{},\n",
    "                'Asian':{},\n",
    "                'African':{},\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_class_feature_dict['Indian'] = {'a':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indian': {'a': 1}, 'Caucasian': {}, 'Asian': {}, 'African': {}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_class_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
